# R--Red-Wine-Prediction

This repository will contain different predictive methods of the red wine ratings (along with my own explanations!) based on various features. Data can be found as *red.txt* under the main directory.

Content:

1. **Decision Tree** (packages: rpart, caret) <br/>
2. 

Setup
=====

R package caret (Classification And REgression Training) will mainly be used to train our models in this repository.

Data Checking
=============

Overview the data with str() and check that no missing values are present.

``` r
str(red.df)
```

    ## 'data.frame':    1599 obs. of  12 variables:
    ##  $ fixed.acidity       : num  7.4 7.8 7.8 11.2 7.4 7.4 7.9 7.3 7.8 7.5 ...
    ##  $ volatile.acidity    : num  0.7 0.88 0.76 0.28 0.7 0.66 0.6 0.65 0.58 0.5 ...
    ##  $ citric.acid         : num  0 0 0.04 0.56 0 0 0.06 0 0.02 0.36 ...
    ##  $ residual.sugar      : num  1.9 2.6 2.3 1.9 1.9 1.8 1.6 1.2 2 6.1 ...
    ##  $ chlorides           : num  0.076 0.098 0.092 0.075 0.076 0.075 0.069 0.065 0.073 0.071 ...
    ##  $ free.sulfur.dioxide : num  11 25 15 17 11 13 15 15 9 17 ...
    ##  $ total.sulfur.dioxide: num  34 67 54 60 34 40 59 21 18 102 ...
    ##  $ density             : num  0.998 0.997 0.997 0.998 0.998 ...
    ##  $ pH                  : num  3.51 3.2 3.26 3.16 3.51 3.51 3.3 3.39 3.36 3.35 ...
    ##  $ sulphates           : num  0.56 0.68 0.65 0.58 0.56 0.56 0.46 0.47 0.57 0.8 ...
    ##  $ alcohol             : num  9.4 9.8 9.8 9.8 9.4 9.4 9.4 10 9.5 10.5 ...
    ##  $ quality             : int  5 5 5 6 5 5 5 7 7 5 ...

``` r
any(is.na(red.df)) # checking missing values
```

    ## [1] FALSE

Splitting
=========

Split the dataset into 90% & 10% for training & test sets.

``` r
index <- sample(nrow(red.df)*0.1) # random index for test set

train <- red.df[-index, ]
test <- red.df[index, ]
```

Modelling
=========

The train() function from the caret package trains the model with given arguments. According to the method used, specific tuning parameters will be required to tune the model. Here we have rpart requiring cp (Complexity Parameter) as its only parameter. A grid of cp can be fed to the argument tuneGrid for search of best result (E.g. Choosing the value of cp giving the lowest RMSE.) trControl specifies the type of resampling.

Decision Tree - Red Wine Prediction
================

We will start with building a Decision Tree Model.

``` r
grid.rt <- expand.grid(.cp = seq(0.001, 0.1, by = 0.001))
grid.rt # grid of tuning paramters
```

    ##       .cp
    ## 1   0.001
    ## 2   0.002
    ## 3   0.003
    ## 4   0.004
    ## 5   0.005
    ## 6   0.006
    ## 7   0.007
    ## 8   0.008
    ## 9   0.009
    ## 10  0.010
    ## 11  0.011
    ## 12  0.012
    ## 13  0.013
    ## 14  0.014
    ## 15  0.015
    ## 16  0.016
    ## 17  0.017
    ## 18  0.018
    ## 19  0.019
    ## 20  0.020
    ## 21  0.021
    ## 22  0.022
    ## 23  0.023
    ## 24  0.024
    ## 25  0.025
    ## 26  0.026
    ## 27  0.027
    ## 28  0.028
    ## 29  0.029
    ## 30  0.030
    ## 31  0.031
    ## 32  0.032
    ## 33  0.033
    ## 34  0.034
    ## 35  0.035
    ## 36  0.036
    ## 37  0.037
    ## 38  0.038
    ## 39  0.039
    ## 40  0.040
    ## 41  0.041
    ## 42  0.042
    ## 43  0.043
    ## 44  0.044
    ## 45  0.045
    ## 46  0.046
    ## 47  0.047
    ## 48  0.048
    ## 49  0.049
    ## 50  0.050
    ## 51  0.051
    ## 52  0.052
    ## 53  0.053
    ## 54  0.054
    ## 55  0.055
    ## 56  0.056
    ## 57  0.057
    ## 58  0.058
    ## 59  0.059
    ## 60  0.060
    ## 61  0.061
    ## 62  0.062
    ## 63  0.063
    ## 64  0.064
    ## 65  0.065
    ## 66  0.066
    ## 67  0.067
    ## 68  0.068
    ## 69  0.069
    ## 70  0.070
    ## 71  0.071
    ## 72  0.072
    ## 73  0.073
    ## 74  0.074
    ## 75  0.075
    ## 76  0.076
    ## 77  0.077
    ## 78  0.078
    ## 79  0.079
    ## 80  0.080
    ## 81  0.081
    ## 82  0.082
    ## 83  0.083
    ## 84  0.084
    ## 85  0.085
    ## 86  0.086
    ## 87  0.087
    ## 88  0.088
    ## 89  0.089
    ## 90  0.090
    ## 91  0.091
    ## 92  0.092
    ## 93  0.093
    ## 94  0.094
    ## 95  0.095
    ## 96  0.096
    ## 97  0.097
    ## 98  0.098
    ## 99  0.099
    ## 100 0.100

``` r
trControl <- trainControl(method = "cv", number = 10) # 10-fold Cross Validation

rtCV <- train(quality ~ ., data = train, # model training
              method = "rpart",
              tuneGrid = grid.rt,
              trControl = trControl)
```

Model Checking
==============

Checking the model rtCV and its plot, we see that the lowest RMSE happens at cp = 0.005.

``` r
rtCV
```

    ## CART 
    ## 
    ## 1440 samples
    ##   11 predictor
    ## 
    ## No pre-processing
    ## Resampling: Cross-Validated (10 fold) 
    ## Summary of sample sizes: 1297, 1296, 1295, 1296, 1296, 1297, ... 
    ## Resampling results across tuning parameters:
    ## 
    ##   cp     RMSE       Rsquared   MAE      
    ##   0.001  0.6823074  0.3509633  0.4944652
    ##   0.002  0.6779104  0.3481171  0.5017824
    ##   0.003  0.6626309  0.3627575  0.5060709
    ##   0.004  0.6686366  0.3471247  0.5161070
    ##   0.005  0.6691563  0.3417418  0.5282843
    ##   0.006  0.6654959  0.3452012  0.5274759
    ##   0.007  0.6677282  0.3390463  0.5287621
    ##   0.008  0.6716171  0.3293876  0.5332523
    ##   0.009  0.6747072  0.3228688  0.5365558
    ##   0.010  0.6738002  0.3242500  0.5363685
    ##   0.011  0.6734717  0.3251538  0.5361592
    ##   0.012  0.6736046  0.3245963  0.5362509
    ##   0.013  0.6760993  0.3197633  0.5375402
    ##   0.014  0.6798686  0.3120764  0.5399021
    ##   0.015  0.6821410  0.3069735  0.5367299
    ##   0.016  0.6853884  0.2997186  0.5376692
    ##   0.017  0.6853884  0.2997186  0.5376692
    ##   0.018  0.6827234  0.3047637  0.5363793
    ##   0.019  0.6831658  0.3039316  0.5387201
    ##   0.020  0.6836238  0.3029845  0.5388903
    ##   0.021  0.6825532  0.3052877  0.5368410
    ##   0.022  0.6869586  0.2966875  0.5395601
    ##   0.023  0.6899150  0.2909271  0.5429047
    ##   0.024  0.6923686  0.2860245  0.5460339
    ##   0.025  0.6944584  0.2821131  0.5496505
    ##   0.026  0.6990052  0.2726946  0.5544412
    ##   0.027  0.7011112  0.2683008  0.5570431
    ##   0.028  0.7009296  0.2685251  0.5575200
    ##   0.029  0.7069491  0.2563972  0.5643582
    ##   0.030  0.7141889  0.2413102  0.5686806
    ##   0.031  0.7141889  0.2413102  0.5686806
    ##   0.032  0.7182710  0.2326317  0.5738887
    ##   0.033  0.7182710  0.2326317  0.5738887
    ##   0.034  0.7167378  0.2336645  0.5733348
    ##   0.035  0.7167378  0.2336645  0.5733348
    ##   0.036  0.7230403  0.2203651  0.5858482
    ##   0.037  0.7305449  0.2052599  0.5958665
    ##   0.038  0.7305449  0.2052599  0.5958665
    ##   0.039  0.7293321  0.2068247  0.5961414
    ##   0.040  0.7306226  0.2030868  0.5976703
    ##   0.041  0.7306226  0.2030868  0.5976703
    ##   0.042  0.7309003  0.2026608  0.5998986
    ##   0.043  0.7309003  0.2026608  0.5998986
    ##   0.044  0.7309003  0.2026608  0.5998986
    ##   0.045  0.7309003  0.2026608  0.5998986
    ##   0.046  0.7309003  0.2026608  0.5998986
    ##   0.047  0.7309003  0.2026608  0.5998986
    ##   0.048  0.7309003  0.2026608  0.5998986
    ##   0.049  0.7309003  0.2026608  0.5998986
    ##   0.050  0.7309003  0.2026608  0.5998986
    ##   0.051  0.7309003  0.2026608  0.5998986
    ##   0.052  0.7309003  0.2026608  0.5998986
    ##   0.053  0.7309003  0.2026608  0.5998986
    ##   0.054  0.7309003  0.2026608  0.5998986
    ##   0.055  0.7345024  0.1950486  0.5983339
    ##   0.056  0.7344794  0.1950486  0.5971806
    ##   0.057  0.7344794  0.1950486  0.5971806
    ##   0.058  0.7354598  0.1940798  0.5940867
    ##   0.059  0.7354598  0.1940798  0.5940867
    ##   0.060  0.7406611  0.1815848  0.5806754
    ##   0.061  0.7386125  0.1839177  0.5779709
    ##   0.062  0.7391197  0.1824238  0.5747936
    ##   0.063  0.7391197  0.1824238  0.5747936
    ##   0.064  0.7391197  0.1824238  0.5747936
    ##   0.065  0.7391197  0.1824238  0.5747936
    ##   0.066  0.7391197  0.1824238  0.5747936
    ##   0.067  0.7391197  0.1824238  0.5747936
    ##   0.068  0.7391197  0.1824238  0.5747936
    ##   0.069  0.7391197  0.1824238  0.5747936
    ##   0.070  0.7391197  0.1824238  0.5747936
    ##   0.071  0.7391197  0.1824238  0.5747936
    ##   0.072  0.7391197  0.1824238  0.5747936
    ##   0.073  0.7391197  0.1824238  0.5747936
    ##   0.074  0.7391197  0.1824238  0.5747936
    ##   0.075  0.7391197  0.1824238  0.5747936
    ##   0.076  0.7391197  0.1824238  0.5747936
    ##   0.077  0.7391197  0.1824238  0.5747936
    ##   0.078  0.7391197  0.1824238  0.5747936
    ##   0.079  0.7391197  0.1824238  0.5747936
    ##   0.080  0.7391197  0.1824238  0.5747936
    ##   0.081  0.7391197  0.1824238  0.5747936
    ##   0.082  0.7391197  0.1824238  0.5747936
    ##   0.083  0.7391197  0.1824238  0.5747936
    ##   0.084  0.7391197  0.1824238  0.5747936
    ##   0.085  0.7391197  0.1824238  0.5747936
    ##   0.086  0.7391197  0.1824238  0.5747936
    ##   0.087  0.7468850  0.1667631  0.5842336
    ##   0.088  0.7468850  0.1667631  0.5842336
    ##   0.089  0.7468850  0.1667631  0.5842336
    ##   0.090  0.7468850  0.1667631  0.5842336
    ##   0.091  0.7468850  0.1667631  0.5842336
    ##   0.092  0.7468850  0.1667631  0.5842336
    ##   0.093  0.7468850  0.1667631  0.5842336
    ##   0.094  0.7468850  0.1667631  0.5842336
    ##   0.095  0.7468850  0.1667631  0.5842336
    ##   0.096  0.7468850  0.1667631  0.5842336
    ##   0.097  0.7468850  0.1667631  0.5842336
    ##   0.098  0.7468850  0.1667631  0.5842336
    ##   0.099  0.7468850  0.1667631  0.5842336
    ##   0.100  0.7468850  0.1667631  0.5842336
    ## 
    ## RMSE was used to select the optimal model using the smallest value.
    ## The final value used for the model was cp = 0.003.

``` r
plot(rtCV)
```

![](/Decision_Tree/Decision_Tree_with_caret_files/figure-markdown_github/unnamed-chunk-5-1.png)

Prediction and evaluation
=========================

Final step: Making prediction with the test set on-hold. Check out the MSE (mean squared error)--the mean of squared distance between each predicted and the original value.

``` r
pred <- predict(rtCV, newdata = test)
head(pred) # prediction
```

    ##      105       43       55       50       78       90 
    ## 5.093750 5.181818 5.411255 5.102564 5.365854 5.411255

``` r
head(test$quality) # original
```

    ## [1] 5 6 6 5 6 5

``` r
MSE <- mean((pred - test$quality)^2)
MSE
```

    ## [1] 0.4174422
